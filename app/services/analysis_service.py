"""
èƒŒè¯µåˆ†ææœåŠ¡
åŸºäºASRå’Œå‘éŸ³è¯„ä¼°ç®—æ³•çš„æ™ºèƒ½èƒŒè¯µåˆ†æç³»ç»Ÿ
"""
import re
import time
import logging
from typing import List, Optional, Dict, Tuple
from difflib import SequenceMatcher
import aiohttp
from fastapi import HTTPException

from app.core.config import settings
from app.services.media_service import MediaProcessingService
from app.schemas.analysis import (
    RecitationAnalysisRequest,
    RecitationAnalysisResponse,
    RecitationAnalysis,
    RecitationScores,
    WordScore,
    ASRResponse,
    ASRWord,
    CloudflareASRResponse,
    WordAlignment,
    WhisperASRResponse
)

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class PreprocessedText:
    """é¢„å¤„ç†åçš„æ–‡æœ¬æ•°æ®ç»“æ„"""
    def __init__(self, processed_words: List[str], original_mapping: Dict[str, str]):
        self.processed_words = processed_words  # å¤„ç†åçš„åˆ†è¯åˆ—è¡¨
        self.original_mapping = original_mapping  # å¤„ç†ååˆ†è¯åˆ°åŸå§‹åˆ†è¯çš„æ˜ å°„


class RecitationAnalysisService:
    """èƒŒè¯µåˆ†ææœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.session: Optional[aiohttp.ClientSession] = None
        self.media_service = MediaProcessingService()
    
    async def analyze_recitation(self, request: RecitationAnalysisRequest) -> RecitationAnalysisResponse:
        """
        åˆ†æç”¨æˆ·çš„èƒŒè¯µæƒ…å†µ
        
        Args:
            request: åˆ†æè¯·æ±‚ï¼ŒåŒ…å«åŸå§‹æ–‡æœ¬å’ŒéŸ³é¢‘URL
            
        Returns:
            RecitationAnalysisResponse: å®Œæ•´çš„åˆ†æç»“æœ
        """
        start_time = time.time()
        logger.info(f"å¼€å§‹èƒŒè¯µåˆ†æ - éŸ³é¢‘URL: {request.audio_url}")
        
        try:
            # 1. è°ƒç”¨ASRæ¥å£è·å–è½¬å½•æ–‡æœ¬
            asr_result = await self._call_asr_api(request.audio_url, request.language)
            logger.info(f"ASRè¯†åˆ«å®Œæˆ - åŸå§‹æ–‡æœ¬: '{request.original_text}', è¯†åˆ«æ–‡æœ¬: '{asr_result.text}', è¯†åˆ«è¯æ•°: {asr_result.word_count}")
            
            # 2. é¢„å¤„ç†æ–‡æœ¬ï¼Œè·å–å¤„ç†ååˆ†è¯å’ŒåŸå§‹æ˜ å°„
            original_preprocessed = self._preprocess_text(request.original_text)
            recognized_preprocessed = self._preprocess_text(asr_result.text)
            
            # 3. è¿›è¡Œè¯è¯­å¯¹é½
            word_alignments = self._align_words(
                original_preprocessed.processed_words, 
                recognized_preprocessed.processed_words, 
                asr_result.words
            )
            
            # 4. è®¡ç®—å„ç§è¯„åˆ†
            analysis = self._calculate_analysis_scores(
                original_preprocessed, 
                recognized_preprocessed, 
                word_alignments, 
                asr_result
            )
            
            processing_time = time.time() - start_time
            logger.info(f"èƒŒè¯µåˆ†æå®Œæˆ - ç”¨æ—¶: {processing_time:.2f}s")
            
            return RecitationAnalysisResponse(
                success=True,
                analysis=analysis,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"èƒŒè¯µåˆ†æå¤±è´¥ - ç”¨æ—¶: {processing_time:.2f}s, é”™è¯¯: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"èƒŒè¯µåˆ†æå¤±è´¥: {str(e)}"
            )
    
    async def _call_asr_api(self, audio_url: str, language: str = "zh") -> ASRResponse:
        """
        è°ƒç”¨æ–°çš„Whisper ASR API
        
        Args:
            audio_url: éŸ³é¢‘æ–‡ä»¶URLï¼ˆå¿…é¡»æ˜¯MP3æ ¼å¼ï¼‰
            language: è¯­è¨€ä»£ç ï¼ˆzhä¸­æ–‡ï¼Œenè‹±æ–‡ï¼‰
            
        Returns:
            ASRResponse: ASRè¯†åˆ«ç»“æœ
        """
        try:
            logger.info("å¼€å§‹ASRéŸ³é¢‘å¤„ç†")
            
            # åˆ›å»ºä¼šè¯ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=settings.analysis_timeout)
                )
            
            # ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
            async with self.session.get(audio_url) as audio_response:
                if audio_response.status != 200:
                    raise Exception(f"æ— æ³•ä¸‹è½½éŸ³é¢‘æ–‡ä»¶: HTTP {audio_response.status}")
                audio_data = await audio_response.read()

            # é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼šæ£€æµ‹æ–‡ä»¶ç±»å‹ï¼Œè‹¥ä¸ºè§†é¢‘åˆ™è½¬ç ä¸ºéŸ³é¢‘ï¼Œå°†å¤šå£°é“éŸ³é¢‘è½¬æ¢ä¸ºå•å£°é“
            try:
                processed_audio_data = await self.media_service.preprocess_audio_for_asr(
                    audio_data, audio_url
                )
                logger.info("éŸ³é¢‘é¢„å¤„ç†å®Œæˆ")
            except Exception as e:
                # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                logger.warning(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {str(e)}")
                processed_audio_data = audio_data
            
            # ä¸Šä¼ é¢„å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶åˆ°COS
            processed_audio_url = await self._upload_processed_audio(processed_audio_data)
            
            # è°ƒç”¨æ–°çš„Whisper ASR APIï¼ˆGETè¯·æ±‚ï¼‰
            logger.info("å¼€å§‹è°ƒç”¨Whisper ASR API")
            async with self.session.get(settings.whisper_api_url, params={
                "url": processed_audio_url
            }) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"ASR APIè°ƒç”¨å¤±è´¥: HTTP {response.status}, {error_text}")
                
                response_data = await response.json()
                
                # è§£ææ–°APIçš„å“åº”æ ¼å¼
                whisper_response = WhisperASRResponse(**response_data)
                
                # è½¬æ¢ä¸ºç»Ÿä¸€çš„ASRResponseæ ¼å¼
                asr_words = []
                
                # ä»segmentsä¸­æå–æ‰€æœ‰å•è¯ä¿¡æ¯
                for segment in whisper_response.segments:
                    for word_data in segment.words:
                        asr_words.append(ASRWord(
                            word=word_data.word,
                            start=word_data.start,
                            end=word_data.end,
                            confidence=None  # æ–°APIä¸æä¾›ç½®ä¿¡åº¦ä¿¡æ¯
                        ))
                
                logger.info(f"ASR APIè°ƒç”¨æˆåŠŸ - è¯†åˆ«è¯æ•°: {whisper_response.word_count}")
                
                return ASRResponse(
                    text=whisper_response.text,
                    word_count=whisper_response.word_count,
                    words=asr_words,
                    vtt=whisper_response.vtt
                )
                
        except Exception as e:
            logger.error(f"ASRå¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise Exception(f"ASRå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _upload_processed_audio(self, audio_data: bytes) -> str:
        """
        ä¸Šä¼ é¢„å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶åˆ°è…¾è®¯äº‘COS
        
        Args:
            audio_data: é¢„å¤„ç†åçš„éŸ³é¢‘æ•°æ®
            
        Returns:
            str: ä¸Šä¼ åçš„éŸ³é¢‘æ–‡ä»¶URL
        """
        import tempfile
        import os
        from datetime import datetime
        
        temp_file = None
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨WAVæ ¼å¼ï¼Œå› ä¸ºé¢„å¤„ç†åæ˜¯WAVæ ¼å¼ï¼‰
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_file_path = temp_file.name
            
            # ç”Ÿæˆå½“å‰æœˆä»½çš„æ–‡ä»¶å¤¹è·¯å¾„
            current_month = datetime.now().strftime("%Y-%m")
            storage_path = f"temp_preprocess_files/{current_month}"
            
            # ä¸Šä¼ åˆ°è…¾è®¯äº‘COS
            from pathlib import Path
            upload_url = await self.media_service._upload_to_cos(
                file_path=Path(temp_file_path),
                file_type="audio",
                original_content_type="audio/wav",
                storage_path=storage_path
            )
            
            logger.info("é¢„å¤„ç†éŸ³é¢‘ä¸Šä¼ åˆ°COSå®Œæˆ")
            return upload_url
            
        except Exception as e:
            logger.error(f"é¢„å¤„ç†éŸ³é¢‘ä¸Šä¼ åˆ°COSå¤±è´¥: {str(e)}", exc_info=True)
            raise
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
    
    def _preprocess_text(self, text: str) -> PreprocessedText:
        """
        æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…ç†æ ‡ç‚¹ç¬¦å·ï¼Œåˆ†è¯ï¼Œå¹¶ç»´æŠ¤åŸå§‹åˆ†è¯æ˜ å°„
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            PreprocessedText: åŒ…å«å¤„ç†ååˆ†è¯å’ŒåŸå§‹æ˜ å°„çš„æ•°æ®ç»“æ„
        """
        # å…ˆè¿›è¡ŒåŸå§‹åˆ†è¯ï¼ˆä¿ç•™æ ‡ç‚¹ç¬¦å·å’Œå¤§å°å†™ï¼‰
        if self._is_chinese_text(text):
            # ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²
            original_words = [char for char in text if char.strip()]
        else:
            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²
            original_words = text.split()
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        original_words = [word for word in original_words if word.strip()]
        
        # å¯¹æ¯ä¸ªåŸå§‹è¯è¿›è¡Œé¢„å¤„ç†
        processed_words = []
        original_mapping = {}
        
        for original_word in original_words:
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            processed_word = re.sub(r'[^\w\s\u4e00-\u9fff]', '', original_word)
            # è½¬æ¢ä¸ºå°å†™ï¼ˆå¯¹è‹±æ–‡ï¼‰
            processed_word = processed_word.lower()
            
            # åªä¿ç•™éç©ºçš„å¤„ç†åè¯è¯­
            if processed_word.strip():
                processed_words.append(processed_word)
                original_mapping[processed_word] = original_word
        
        return PreprocessedText(processed_words, original_mapping)
    
    def _is_chinese_text(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        chinese_count = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len([char for char in text if char.strip()])
        return chinese_count > total_chars * 0.5 if total_chars > 0 else False
    
    def _align_words(
        self, 
        expected_words: List[str], 
        actual_words: List[str], 
        asr_words: List[ASRWord]
    ) -> List[WordAlignment]:
        """
        è¯è¯­å¯¹é½ç®—æ³•ï¼ˆåŸºäºç¼–è¾‘è·ç¦»å’Œç›¸ä¼¼åº¦ï¼‰
        
        Args:
            expected_words: æœŸæœ›çš„å•è¯åˆ—è¡¨
            actual_words: å®é™…è¯†åˆ«çš„å•è¯åˆ—è¡¨
            asr_words: ASRæä¾›çš„å¸¦æ—¶é—´æˆ³çš„å•è¯ä¿¡æ¯
            
        Returns:
            List[WordAlignment]: å¯¹é½ç»“æœ
        """
        alignments = []
        
        # åˆ›å»ºæ—¶é—´æˆ³å­—å…¸
        word_timestamps: Dict[str, Dict[str, float]] = {}
        for asr_word in asr_words:
            clean_word = re.sub(r'[^\w\s\u4e00-\u9fff]', '', asr_word.word.lower())
            word_timestamps[clean_word] = {
                'start': float(asr_word.start),
                'end': float(asr_word.end)
            }
        
        # ä½¿ç”¨åŠ¨æ€è§„åˆ’è¿›è¡Œåºåˆ—å¯¹é½
        dp = [[0.0] * (len(actual_words) + 1) for _ in range(len(expected_words) + 1)]
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        for i in range(1, len(expected_words) + 1):
            for j in range(1, len(actual_words) + 1):
                similarity = self._calculate_word_similarity(
                    expected_words[i-1], 
                    actual_words[j-1]
                )
                
                # ä¸‰ç§æ“ä½œï¼šåŒ¹é…ã€åˆ é™¤ã€æ’å…¥
                match_score = dp[i-1][j-1] + similarity
                delete_score = dp[i-1][j] - 0.5
                insert_score = dp[i][j-1] - 0.5
                
                dp[i][j] = max(match_score, delete_score, insert_score)
        
        # å›æº¯æ‰¾åˆ°æœ€ä¼˜å¯¹é½è·¯å¾„
        i, j = len(expected_words), len(actual_words)
        while i > 0 and j > 0:
            expected_word = expected_words[i-1]
            actual_word = actual_words[j-1]
            similarity = self._calculate_word_similarity(expected_word, actual_word)
            
            # è·å–æ—¶é—´æˆ³
            timestamps = word_timestamps.get(actual_word, {'start': 0.0, 'end': 0.0})
            
            if abs(dp[i][j] - (dp[i-1][j-1] + similarity)) < 1e-6:
                # åŒ¹é…
                alignments.append(WordAlignment(
                    expected_word=expected_word,
                    actual_word=actual_word,
                    similarity=similarity,
                    start_time=timestamps['start'],
                    end_time=timestamps['end'],
                    is_match=similarity >= settings.min_word_similarity
                ))
                i -= 1
                j -= 1
            elif abs(dp[i][j] - (dp[i-1][j] - 0.5)) < 1e-6:
                # åˆ é™¤ï¼ˆé—æ¼ï¼‰
                alignments.append(WordAlignment(
                    expected_word=expected_word,
                    actual_word="",
                    similarity=0.0,
                    start_time=0.0,
                    end_time=0.0,
                    is_match=False
                ))
                i -= 1
            else:
                # æ’å…¥ï¼ˆå¤šä½™ï¼‰
                timestamps = word_timestamps.get(actual_word, {'start': 0.0, 'end': 0.0})
                alignments.append(WordAlignment(
                    expected_word="",
                    actual_word=actual_word,
                    similarity=0.0,
                    start_time=timestamps['start'],
                    end_time=timestamps['end'],
                    is_match=False
                ))
                j -= 1
        
        # å¤„ç†å‰©ä½™çš„è¯
        while i > 0:
            alignments.append(WordAlignment(
                expected_word=expected_words[i-1],
                actual_word="",
                similarity=0.0,
                start_time=0.0,
                end_time=0.0,
                is_match=False
            ))
            i -= 1
        
        while j > 0:
            actual_word = actual_words[j-1]
            timestamps = word_timestamps.get(actual_word, {'start': 0.0, 'end': 0.0})
            alignments.append(WordAlignment(
                expected_word="",
                actual_word=actual_word,
                similarity=0.0,
                start_time=timestamps['start'],
                end_time=timestamps['end'],
                is_match=False
            ))
            j -= 1
        
        return list(reversed(alignments))
    
    def _calculate_word_similarity(self, word1: str, word2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå•è¯çš„ç›¸ä¼¼åº¦
        
        Args:
            word1: å•è¯1
            word2: å•è¯2
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        if word1 == word2:
            return 1.0
        
        # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
        matcher = SequenceMatcher(None, word1.lower(), word2.lower())
        return matcher.ratio()
    
    def _calculate_analysis_scores(
        self, 
        expected_preprocessed: PreprocessedText, 
        recognized_preprocessed: PreprocessedText, 
        word_alignments: List[WordAlignment],
        asr_result: ASRResponse
    ) -> RecitationAnalysis:
        """
        è®¡ç®—å„ç§åˆ†æè¯„åˆ†
        
        Args:
            expected_preprocessed: é¢„å¤„ç†åçš„æœŸæœ›å•è¯åˆ—è¡¨å’ŒåŸå§‹æ˜ å°„
            recognized_preprocessed: é¢„å¤„ç†åçš„è¯†åˆ«å•è¯åˆ—è¡¨å’ŒåŸå§‹æ˜ å°„
            word_alignments: è¯è¯­å¯¹é½ç»“æœ
            asr_result: ASRç»“æœ
            
        Returns:
            RecitationAnalysis: å®Œæ•´åˆ†æç»“æœ
        """
        # ç»Ÿè®¡åŸºæœ¬æŒ‡æ ‡
        total_words = len(expected_preprocessed.processed_words)
        correct_words = sum(1 for alignment in word_alignments if alignment.is_match)
        word_accuracy = (correct_words / total_words * 100) if total_words > 0 else 0
        
        # ç”Ÿæˆå•è¯è¯¦ç»†åˆ†æ
        word_details = []
        mispronounced_words = []
        missing_words = []
        extra_words = []
        
        for alignment in word_alignments:
            if alignment.expected_word and alignment.actual_word:
                # åŒ¹é…çš„è¯ - ä½¿ç”¨åŸå§‹åˆ†è¯
                score = alignment.similarity * 100
                is_correct = alignment.is_match
                
                # ä»æ˜ å°„ä¸­è·å–åŸå§‹åˆ†è¯
                original_expected = expected_preprocessed.original_mapping.get(alignment.expected_word, alignment.expected_word)
                original_actual = recognized_preprocessed.original_mapping.get(alignment.actual_word, alignment.actual_word)
                
                word_details.append(WordScore(
                    word=original_expected,  # ä½¿ç”¨åŸå§‹åˆ†è¯
                    expected=original_expected,  # ä½¿ç”¨åŸå§‹åˆ†è¯
                    actual=original_actual,  # ä½¿ç”¨åŸå§‹åˆ†è¯
                    score=score,
                    start_time=alignment.start_time,
                    end_time=alignment.end_time,
                    is_correct=is_correct
                ))
                
                if not is_correct:
                    # mispronounced_words ä¹Ÿä½¿ç”¨åŸå§‹åˆ†è¯
                    mispronounced_words.append(original_expected)
                    
            elif alignment.expected_word and not alignment.actual_word:
                # é—æ¼çš„è¯ - ä½¿ç”¨åŸå§‹åˆ†è¯
                original_expected = expected_preprocessed.original_mapping.get(alignment.expected_word, alignment.expected_word)
                missing_words.append(original_expected)
                word_details.append(WordScore(
                    word=original_expected,  # ä½¿ç”¨åŸå§‹åˆ†è¯
                    expected=original_expected,  # ä½¿ç”¨åŸå§‹åˆ†è¯
                    actual="",
                    score=0.0,
                    start_time=0.0,
                    end_time=0.0,
                    is_correct=False
                ))
            elif not alignment.expected_word and alignment.actual_word:
                # å¤šä½™çš„è¯ - ä½¿ç”¨åŸå§‹åˆ†è¯
                original_actual = recognized_preprocessed.original_mapping.get(alignment.actual_word, alignment.actual_word)
                extra_words.append(original_actual)
        
        # è®¡ç®—å„é¡¹è¯„åˆ†
        pronunciation_score = self._calculate_pronunciation_score(word_alignments)
        fluency_score = self._calculate_fluency_score(asr_result, word_alignments)
        accuracy_score = word_accuracy / 10  # è½¬æ¢ä¸º10åˆ†åˆ¶
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        overall_score = (
            pronunciation_score * settings.pronunciation_accuracy_weight +
            fluency_score * settings.fluency_weight +
            accuracy_score * settings.accuracy_weight
        )
        
        scores = RecitationScores(
            accuracy_score=round(accuracy_score, 2),
            fluency_score=round(fluency_score, 2),
            pronunciation_score=round(pronunciation_score, 2),
            overall_score=round(overall_score, 2)
        )
        
        return RecitationAnalysis(
            recognized_text=asr_result.text,
            word_count=total_words,
            correct_words=correct_words,
            word_accuracy=round(word_accuracy, 2),
            scores=scores,
            word_details=word_details,
            phoneme_details=None,  # æš‚ä¸æ”¯æŒéŸ³ç´ çº§åˆ†æ
            mispronounced_words=mispronounced_words,
            missing_words=missing_words,
            extra_words=extra_words
        )
    
    def _calculate_pronunciation_score(self, word_alignments: List[WordAlignment]) -> float:
        """
        è®¡ç®—å‘éŸ³å‡†ç¡®åº¦è¯„åˆ†
        
        Args:
            word_alignments: è¯è¯­å¯¹é½ç»“æœ
            
        Returns:
            float: å‘éŸ³å‡†ç¡®åº¦è¯„åˆ†ï¼ˆ0-10ï¼‰
        """
        if not word_alignments:
            return 0.0
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        matched_alignments = [
            alignment for alignment in word_alignments 
            if alignment.expected_word and alignment.actual_word
        ]
        
        if not matched_alignments:
            return 0.0
        
        avg_similarity = sum(alignment.similarity for alignment in matched_alignments) / len(matched_alignments)
        return min(avg_similarity * 10, 10.0)
    
    def _calculate_fluency_score(self, asr_result: ASRResponse, word_alignments: List[WordAlignment]) -> float:
        """
        è®¡ç®—æµç•…åº¦è¯„åˆ†
        
        Args:
            asr_result: ASRç»“æœ
            word_alignments: è¯è¯­å¯¹é½ç»“æœ
            
        Returns:
            float: æµç•…åº¦è¯„åˆ†ï¼ˆ0-10ï¼‰
        """
        # åŸºäºè¯­é€Ÿå’Œåœé¡¿æ¥è®¡ç®—æµç•…åº¦
        if not asr_result.words:
            return 5.0  # é»˜è®¤åˆ†æ•°
        
        # è®¡ç®—æ€»æ—¶é•¿
        total_duration = asr_result.words[-1].end - asr_result.words[0].start
        
        # è®¡ç®—è¯­é€Ÿï¼ˆè¯/åˆ†é’Ÿï¼‰
        words_per_minute = (len(asr_result.words) / total_duration) * 60
        
        # ç†æƒ³è¯­é€ŸèŒƒå›´ï¼ˆæ ¹æ®è¯­è¨€è°ƒæ•´ï¼‰
        ideal_wpm_min, ideal_wpm_max = 120.0, 180.0  # ä¸­æ–‡å­—ç¬¦/åˆ†é’Ÿ
        
        # è¯­é€Ÿè¯„åˆ†
        if ideal_wpm_min <= words_per_minute <= ideal_wpm_max:
            speed_score = 10.0
        else:
            # åç¦»ç†æƒ³èŒƒå›´çš„æƒ©ç½š
            deviation = min(
                abs(words_per_minute - ideal_wpm_min),
                abs(words_per_minute - ideal_wpm_max)
            )
            speed_score = max(10.0 - (deviation / 20), 1.0)
        
        # åœé¡¿è¯„åˆ†ï¼ˆåŸºäºè¯é—´é—´éš”ï¼‰
        pause_score = self._calculate_pause_score(asr_result.words)
        
        # ç»¼åˆæµç•…åº¦è¯„åˆ†
        fluency_score = (speed_score * 0.7 + pause_score * 0.3)
        return min(fluency_score, 10.0)
    
    def _calculate_pause_score(self, words: List[ASRWord]) -> float:
        """
        è®¡ç®—åœé¡¿è¯„åˆ†
        
        Args:
            words: å¸¦æ—¶é—´æˆ³çš„å•è¯åˆ—è¡¨
            
        Returns:
            float: åœé¡¿è¯„åˆ†ï¼ˆ0-10ï¼‰
        """
        if len(words) < 2:
            return 8.0
        
        # è®¡ç®—è¯é—´é—´éš”
        intervals = []
        for i in range(1, len(words)):
            interval = words[i].start - words[i-1].end
            intervals.append(interval)
        
        # åˆ†æåœé¡¿æ¨¡å¼
        avg_interval = sum(intervals) / len(intervals)
        long_pauses = sum(1 for interval in intervals if interval > 1.0)  # è¶…è¿‡1ç§’çš„åœé¡¿
        
        # è¯„åˆ†é€»è¾‘
        if avg_interval < 0.5 and long_pauses == 0:
            return 10.0
        elif avg_interval < 1.0 and long_pauses <= 2:
            return 8.0
        elif avg_interval < 2.0 and long_pauses <= 5:
            return 6.0
        else:
            return max(4.0 - long_pauses * 0.5, 1.0)
    
    def _generate_ai_feedback(self, analysis: RecitationAnalysis) -> str:
        """
        ç”ŸæˆAIåé¦ˆå»ºè®®
        
        Args:
            analysis: åˆ†æç»“æœ
            
        Returns:
            str: äººæ€§åŒ–çš„åé¦ˆå»ºè®®
        """
        overall_score = analysis.scores.overall_score
        accuracy = analysis.word_accuracy
        
        # åŸºç¡€è¯„ä»·
        if overall_score >= 8.5:
            base_feedback = "ğŸ‰ å¤ªæ£’äº†ï¼ä½ çš„èƒŒè¯µéå¸¸å‡ºè‰²ï¼"
        elif overall_score >= 7.0:
            base_feedback = "ğŸ‘ å¾ˆå¥½ï¼ä½ çš„èƒŒè¯µè´¨é‡å¾ˆä¸é”™ï¼"
        elif overall_score >= 5.5:
            base_feedback = "ğŸ’ª ä¸é”™çš„å°è¯•ï¼è¿˜æœ‰è¿›æ­¥çš„ç©ºé—´ã€‚"
        else:
            base_feedback = "ğŸŒŸ åŠ æ²¹ï¼å¤šç»ƒä¹ ä¸€å®šä¼šæœ‰æå‡çš„ã€‚"
        
        # å…·ä½“å»ºè®®
        suggestions = []
        
        # æ­£ç¡®ç‡å»ºè®®
        if accuracy < 70:
            suggestions.append("å»ºè®®å¤šç†Ÿæ‚‰æ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿å‡†ç¡®è®°å¿†æ¯ä¸ªè¯è¯­")
        elif accuracy < 85:
            suggestions.append("å¯¹ä¸ªåˆ«è¯è¯­çš„å‘éŸ³å¯ä»¥å†åŠ å¼ºç»ƒä¹ ")
        
        # å‘éŸ³å»ºè®®
        if analysis.scores.pronunciation_score < 7:
            suggestions.append("æ³¨æ„å‘éŸ³çš„æ¸…æ™°åº¦ï¼Œå¯ä»¥æ”¾æ…¢é€Ÿåº¦ç¡®ä¿æ¯ä¸ªå­—éŸ³å‡†ç¡®")
            if analysis.mispronounced_words:
                problem_words = analysis.mispronounced_words[:3]  # æœ€å¤šæåŠ3ä¸ª
                suggestions.append(f"ç‰¹åˆ«æ³¨æ„è¿™äº›è¯çš„å‘éŸ³ï¼š{', '.join(problem_words)}")
        
        # æµç•…åº¦å»ºè®®
        if analysis.scores.fluency_score < 7:
            suggestions.append("å¯ä»¥æé«˜è¯­é€Ÿï¼Œå‡å°‘ä¸å¿…è¦çš„åœé¡¿ï¼Œè®©èƒŒè¯µæ›´åŠ æµç•…")
        
        # é—æ¼å’Œå¤šä½™è¯è¯­
        if analysis.missing_words:
            suggestions.append(f"æ³¨æ„ä¸è¦é—æ¼è¿™äº›å†…å®¹ï¼š{', '.join(analysis.missing_words[:2])}")
        
        if analysis.extra_words:
            suggestions.append("å°½é‡é¿å…æ·»åŠ é¢å¤–çš„è¯è¯­ï¼Œä¸¥æ ¼æŒ‰ç…§åŸæ–‡èƒŒè¯µ")
        
        # ç»„åˆåé¦ˆ
        feedback = base_feedback
        if suggestions:
            feedback += "\n\nğŸ“ æ”¹è¿›å»ºè®®ï¼š\n" + "\n".join(f"â€¢ {suggestion}" for suggestion in suggestions[:4])
        
        # é¼“åŠ±è¯­
        feedback += f"\n\nğŸ“Š æœ¬æ¬¡å¾—åˆ†ï¼š{overall_score}/10åˆ†ï¼Œç»§ç»­åŠªåŠ›ï¼Œä½ ä¸€å®šèƒ½åšå¾—æ›´å¥½ï¼"
        
        return feedback
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
            self.session = None